{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1234f893",
   "metadata": {},
   "source": [
    "1. Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html . How does it work? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452aa188",
   "metadata": {},
   "source": [
    "- PyTorch optimizers are useful for minimizing the error rate while training the neural networks. \n",
    "\n",
    "\n",
    "#### When neural network training is taking place... \n",
    "    - The weights or strength of the connection between units of the network, are first randomly initialized. \n",
    "    - Then these weights are updated in each iteration (each epoch) so that they increase accuracy and decrease the error(loss) untill eventually we get good weights.  \n",
    "    \n",
    "    \n",
    "#### In each epoch... \n",
    "    - The output of the training data is compared to actual data by.. \n",
    "    - Calculating the error using the loss function and then the weight is updated accordingly to that error. \n",
    "\n",
    "\n",
    "#### Adam Optimizer\n",
    "Adam Optimizer updates network weights iteratively in training data by using both momentum and adaptive learning rate as opposed to using just a single learning rate for all the weights. So the learning rate changes during the training process. \n",
    "\n",
    "#### Review on Stochatic Gradient Descent\n",
    "- A iterative algorithm that...\n",
    "        \n",
    "        - randomly picks 1 sample per step to calculate the slopes to predict values.\n",
    "        -OR-\n",
    "        - the algorithm starts from a random point on the function and travels down its slope in steps untill eventually it gets to the lowest point of that function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6efa2faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "diabetes_df = pd.read_csv(\"diabetes copy2.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd7c6e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diabetes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b74ebdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4564498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93138344  2.0179454   0.78066953 ...  0.43148259 -0.37477883\n",
      "   0.63212912]\n",
      " [ 0.63260632 -1.14861888  0.46538785 ... -0.1198324  -0.29416766\n",
      "   0.71699246]\n",
      " [-0.56250219 -0.47692343 -0.2702694  ... -0.20958135  2.74517192\n",
      "   0.03808578]\n",
      " ...\n",
      " [-0.86127931 -0.76479291  0.04501228 ...  0.76483585 -0.78380586\n",
      "  -0.30136756]\n",
      " [ 0.63260632  2.20985838  1.2010451  ...  0.43148259 -0.60466993\n",
      "   2.75371249]\n",
      " [ 0.03505207  0.73852549 -0.58555107 ... -0.33779414 -0.57779954\n",
      "   0.29267578]]\n",
      "[1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1\n",
      " 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1\n",
      " 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6d57d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9314,  2.0179,  0.7807,  ...,  0.4315, -0.3748,  0.6321],\n",
      "        [ 0.6326, -1.1486,  0.4654,  ..., -0.1198, -0.2942,  0.7170],\n",
      "        [-0.5625, -0.4769, -0.2703,  ..., -0.2096,  2.7452,  0.0381],\n",
      "        ...,\n",
      "        [-0.8613, -0.7648,  0.0450,  ...,  0.7648, -0.7838, -0.3014],\n",
      "        [ 0.6326,  2.2099,  1.2010,  ...,  0.4315, -0.6047,  2.7537],\n",
      "        [ 0.0351,  0.7385, -0.5856,  ..., -0.3378, -0.5778,  0.2927]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #where the activation functions are\n",
    "\n",
    "#create tensors = matrices \n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df8666c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of nodes (hidden1 and hidden2)(preceptons?)\n",
    "\n",
    "\n",
    "\n",
    "#artificial neural network\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8,hidden1=20,hidden2=20,out_features=2):\n",
    "        super().__init__() #super is a computed indirect reference. So, it isolates changes\n",
    "        # and makes sure that children in the layers of multiple inheritence are calling\n",
    "        #the right parents\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7654973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#create instance of model\n",
    "ann = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7af37f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea70405",
   "metadata": {},
   "source": [
    "Try at least one other optimization function with the diabetes dataset shown in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6891896",
   "metadata": {},
   "source": [
    "Adaptive Gradient Algorithm\n",
    "\n",
    "- AdaGrad’s learning rate (for each parameter) helps increases the learning rate for sparser parameters. Thus, AdaGrad works well for thinly distributed gradients since the algorithm performs smaller updates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77eb8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optimizer = torch.optim.Adagrad(ann.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf35a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.647470235824585\n",
      "Epoch number: 11 with loss: 0.5630926489830017\n",
      "Epoch number: 21 with loss: 0.5061333775520325\n",
      "Epoch number: 31 with loss: 0.47481316328048706\n",
      "Epoch number: 41 with loss: 0.45581328868865967\n",
      "Epoch number: 51 with loss: 0.4437910318374634\n",
      "Epoch number: 61 with loss: 0.4347322881221771\n",
      "Epoch number: 71 with loss: 0.4276331961154938\n",
      "Epoch number: 81 with loss: 0.4215301275253296\n",
      "Epoch number: 91 with loss: 0.41631075739860535\n",
      "Epoch number: 101 with loss: 0.4113055169582367\n",
      "Epoch number: 111 with loss: 0.40628811717033386\n",
      "Epoch number: 121 with loss: 0.401469349861145\n",
      "Epoch number: 131 with loss: 0.3970152735710144\n",
      "Epoch number: 141 with loss: 0.392855167388916\n",
      "Epoch number: 151 with loss: 0.3887157440185547\n",
      "Epoch number: 161 with loss: 0.3847091495990753\n",
      "Epoch number: 171 with loss: 0.3805328607559204\n",
      "Epoch number: 181 with loss: 0.37636613845825195\n",
      "Epoch number: 191 with loss: 0.37252601981163025\n",
      "Epoch number: 201 with loss: 0.36923617124557495\n",
      "Epoch number: 211 with loss: 0.36609312891960144\n",
      "Epoch number: 221 with loss: 0.36308586597442627\n",
      "Epoch number: 231 with loss: 0.36025258898735046\n",
      "Epoch number: 241 with loss: 0.35750728845596313\n",
      "Epoch number: 251 with loss: 0.3549474775791168\n",
      "Epoch number: 261 with loss: 0.3524327576160431\n",
      "Epoch number: 271 with loss: 0.349836528301239\n",
      "Epoch number: 281 with loss: 0.3472379446029663\n",
      "Epoch number: 291 with loss: 0.34462106227874756\n",
      "Epoch number: 301 with loss: 0.34223422408103943\n",
      "Epoch number: 311 with loss: 0.34002333879470825\n",
      "Epoch number: 321 with loss: 0.3377719223499298\n",
      "Epoch number: 331 with loss: 0.33578765392303467\n",
      "Epoch number: 341 with loss: 0.3337673842906952\n",
      "Epoch number: 351 with loss: 0.3318255543708801\n",
      "Epoch number: 361 with loss: 0.32971280813217163\n",
      "Epoch number: 371 with loss: 0.32772573828697205\n",
      "Epoch number: 381 with loss: 0.32589924335479736\n",
      "Epoch number: 391 with loss: 0.3240121603012085\n",
      "Epoch number: 401 with loss: 0.32217714190483093\n",
      "Epoch number: 411 with loss: 0.3204687535762787\n",
      "Epoch number: 421 with loss: 0.3186686038970947\n",
      "Epoch number: 431 with loss: 0.3167886435985565\n",
      "Epoch number: 441 with loss: 0.3149521052837372\n",
      "Epoch number: 451 with loss: 0.3130205273628235\n",
      "Epoch number: 461 with loss: 0.3110172748565674\n",
      "Epoch number: 471 with loss: 0.3091891407966614\n",
      "Epoch number: 481 with loss: 0.30715444684028625\n",
      "Epoch number: 491 with loss: 0.3051953613758087\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss}')\n",
    "        \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() \n",
    "    optimizer.step() #perform one optimization step each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ead86",
   "metadata": {},
   "source": [
    "How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38e2d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02291a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78       150\n",
      "           1       0.60      0.52      0.56        81\n",
      "\n",
      "    accuracy                           0.71       231\n",
      "   macro avg       0.68      0.67      0.67       231\n",
      "weighted avg       0.70      0.71      0.70       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357d72b",
   "metadata": {},
   "source": [
    "Overall, this Adagrad optimizer performed a little better than the Adam optimizer. I think this is because the dataset is not too big so it works well with an optimizer that performs smaller updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94b255",
   "metadata": {},
   "source": [
    "2. Write a function that lists and counts the number of divisors for an input value.\n",
    "    \n",
    "        Example 1:\n",
    "        Input: 5\n",
    "        Output: “There are 2 divisors: 1 and 5”\n",
    "        \n",
    "        Example 2:\n",
    "        Input: 40\n",
    "        Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38f96229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divisor_count (x):\n",
    "    empty_list = []\n",
    "    if x ==1:\n",
    "        print(\"There is 1 divisor only: 1\")\n",
    "    for y in range(1, x+1):\n",
    "        if x % y == 0:\n",
    "            empty_list.append(y)\n",
    "    #print(f'There are {len(empty_list)} divisors: {empty_list}')\n",
    "    print(f'There are {len(empty_list)} divisors: {str((empty_list)[ :-1])[1:-1]} and {empty_list[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e33081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 divisors: 1, 2, 4, 5, 8, 10, 20 and 40\n"
     ]
    }
   ],
   "source": [
    "divisor_count (40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "254736c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 divisors: 1 and 5\n"
     ]
    }
   ],
   "source": [
    "divisor_count (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e7962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
